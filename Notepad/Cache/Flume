# 架构介绍
日志采集工具，主要目的是实时采集本地数据，上传到HDFS或者给Kafka
主要由三部分组成Source + Channel + Sink
不同的需求主要通过对Source与Sink的设置实现

## Source
Source接收服务器或本地传递的数据交付给channel，Source可以处理几乎所有格式的日志数据
## Channel
作为Source与Sink之间的缓冲区，线程安全，允许多个Source或Sink同时读写
Flume自带两种Channel
+ Memeory：缓冲区在内存，断电数据丢失，速度快
+ File：缓冲区在磁盘，断电保留，速度慢
## Sink
从Channel读取数据，将数据传输给目的地

## Event
Flume最小传输单元，结构为K-V对，K存储属性，V存储数据，K称为Header，V称为Body
Sink可以根据Header来决定数据传输的位置

# 常用组件及案例
flume job配置文件怎么写见 https://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html
根据需求挑选合适的sources、channel、sink即可
## 上传Hive.log到HDFS
1. 创建job配置文件
2. 使用创建的配置文件创建agent即可

### 常用Sources
+ exec：用于对某个文件进行跟踪
	+ 会将文件修改内容发送给channel
	+ 不能实现断点续传
+ spooldir：用于对某个文件夹进行跟踪
	+ 不会监控文件修改
	+ 将新增文件发送给channel
+ taildir：跟踪某个文件夹中的文件
	+ 监控文件夹中文件的修改，并将数据发送给channel
	+ 实现断点续传
	+ 默认根据json文件中inode的值与file的值来判断文件是否为新文件（即pos属性）
# 进阶
## 事务
Flume事务有两类，Put事务和Take事务
Put事务，doPut --> doCommit --> doRollback
数据先通过put放入临时缓冲区putList，再通过commit检查channel内存，选择提交或是回滚
Take事务，doTake --> doCommit --> doRollback
数据take到takeList，并发送给HDFS，
commit：若数据全部发送，则清除临时缓冲区takeList，发生异常则回滚，将数据返回给channel

## Agent内部原理
具体内容见pdf，这里给出一些解释
流程为：
Data-->ChannelProcessor-->Interceptor-->ChannelProcessor-->ChannelSelector-->CP-->Channels-->SinkProcessor-->Sinks
拦截器链：对数据进行处理，可以通过拦截器对日志类型进行区分，以便发送给不同的channel
Channel Selector：选择将日志发送给那个channel，往往是根据拦截器添加的header信息决定
SinkProcessor：汇聚channel发送的数据，并选择发送给那个Sink

## Flume拓扑结构
### 简单串联
多个Agent Sink与Source相连，任一Agent挂掉，整个系统就挂了

### 复制和多路复用
通过配置多个Sink与Channel，将数据发送给不同的终点（HDFS，LOGGER，AGENT等等）
一个Sink与一个Channel相连
开启时需要注意先启动Listener再启动server，否则avro无法启动

### 负载均衡和故障转移
单数据源，起始的Agent配置多个Sink，并将每个Sink与另外的Agent相连，再输出到终点，增加容错和数据缓存量

### 聚合
起始有多个数据源，通过多个Agent接收数据，最终聚合到一个Agent上传到终点

## 实际案例
### 多路复用案例
Agent1监控文件变动，并发送给Agent2与Agent3，Agent2将数据发送给HDFS，Agent3发送给LocalFileSystem

### 负载均衡与故障转移
仅测试了故障转移，二者除了group.type与优先级不必设置外，其他设置是一样的，
设置一个Agent1来作为数据源，设置Sink组，组里的每个sink连接到不同的agent即可

### 聚合
设置Agent1与Agent2为数据来源，其sink均设置到agent3所开启的接口，Agent3开启一个Source组，将数据输出到目的地
在这个模式中，agent1与agent2向agent3发送数据，如果a1，a2都用a3的同一个端口，则会在sources进行汇总，若非同一个端口会在channel进行聚合

### 拦截器
按照日志类型，通过拦截器 + 多路复用，将不同的日志发送到不同的channel
自定义拦截器，打包后放到lib目录下

### Source
自定义Source，通过自定义Source，设置不同的数据来源
需要实现configure， process
其中configure实现读取配置文件信息
process实现将source以event形式发送给channel

# 监控工具
一般使用Ganglia监控Flume数据流
## 部署
Ganglia由gmond，gmetad，gweb组成
+ gmond，轻量级服务，安装在需要收集指标数据的节点主机。
+ gmetad，整合所有信息，以RRD格式存储在磁盘
+ gweb，可视化工具

## 监控Flume
可以在页面访问到Ganglia后，启动flume任务，并设置flume.monitoring.type与hosts即可
这部分设置可以添加在flume-env.sh中的JAVA_OPTS中

# 一些概念
## avro
avro是apache开发序列化框架，支持动态类型，支持序列化二进制数据
将消息的schema与body分开
### schema
依赖模式，类似于Class，定义每个实例的属性，主要用json对象来标识


# 源码阅读及源码修改

# 踩坑
## 关于配置文件
这个一定要能复制则复制，是在找不到地方复制了再自己写，太容易丢东西了。。

# 测试修改已有博客内容
