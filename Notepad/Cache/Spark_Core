# Spark部署
Spark拥有更快的速度，它的作业之间的数据通信是基于内存的，只在shuffle的时候将数据写入磁盘，且Spark任务的启动是通过fork线程实现

## Spark主要的部署方式
Spark提供三种部署方式，Spark-Local，Spark-standalone，spark-yarn（不一定非得是yarn，不过yarn国内用的比较多）
对于不同的部署方式，按照文档对其进行不同的配置即可
### spark-local
本地模式执行spark，不需要slave节点
### spark-standalone
独立部署默认，需要slave节点，相当于真实的spark分布式执行，但不使用其他资源管理框架，仅使用spark自身的功能
### spark-yarn
yarn环境下部署spark，国内一般使用此种部署模式，这种情况相当于只是用spark作为计算框架，资源调度由yarn负责

## 其他
Spark还可以在windows下运行，也可以通过k8s进行部署，不过windows下本地运行一般只用来学习，k8s部署尚不成熟，使用较少



# Spark运行架构
Spark运行架构由三部分组成：Driver -> Cluster Manager -> Executor
作为标准的master-slaves架构，其master为driver，slaves为executor
## 核心组件
Spark运行的核心组件为Driver和Executor
### Driver
Driver相当于Spark驱动器，负责运行整个程序，相当于main
### Executor
Worker中的一个进程，负责运行Task

## 其他
### Master
Master在独立部署情况下，负责资源调度
### ApplicationMaster
负责向资源调度器申请Container，并监控任务执行，实现了Driver与ResourceManager的解耦

## 核心概念
### Executor和Core
Core指Executor使用的虚拟Cpu核心数目
### 并行度
整个集群中可__同时__执行的任务数量
### 有向无环图DAG
DAG框架，主要用于任务调度，决定任务调度次序，防止数据流向出现闭环
## 提交流程
1. 任务提交
2. Driver运行→集群管理器→启动Executor
3. 执行main
4. 找到action算子
5. stage划分
6. 创建taskSet→分发给Executor
任务执行有两种部署执行方式：Spark Client和Cluster，区别在于Driver程序运行的节点位置
### Yarn Client
Driver提交的任务在本地机器运行

### Yarn Cluster
由ResourceManager决定在那个Node启动ApplicationMaster（前边提到过，相当于Driver）

# Spark编程中的核心概念
Spark框架提供三个独有的数据结构：
+ RDD：弹性分布式数据集
+ 累加器：分布式环境下的共享写变量
+ 广播变量：分布式环境下的共享只读变量
这三个数据结构是Spark高并发和高吞吐实现的核心

## RDD
RDD用来存放计算逻辑，RDD相当于总的Task，可以将数据分发给不同的Executor执行
### RDD基本操作
+ 转换操作：map、join、groupBy等
	+ 转换操作是惰性的，只有当RDD执行行动操作时才会触发
	+ map与mappartitions区别在于，mappartitions以分区为单位接收数据进行处理，其接收的参数与返回参数都为迭代器，速度较快，但内存占用大，其返回结果并非RDD，而是一个值
	+ sample处理数据倾斜，在进行分区之前，对数据进行抽取，多次抽取后，对大量出现的数据进行处理，防止它们进入同一个分区
	+ partitionBy来分区时，会先判断新的分区器与原本的分区器是否一致，若一致则不进行重分区
	+ reduceByKey和groupByKey的区别
		+ groupByKey与reduceByKey均为宽依赖，需要进行shuffle而shuffle进行操作时数据流不是全部位于内存，父RDD会将数据输出到文件，再让子RDD从文件中读取，因此效率较低
		+ 但groupByKey不会在父RDD进行聚合操作，而是直接进行shuffle，导致需要落盘的数据量较大
		+ reduceByKey则会在父RDD进行聚合操作，相当于combiner，shuffle时落盘的数据量就少
	+ 关于reduceByKey，combinerByKey，aggregateByKey，foldByKey
		+ 这四个都是会在分区内操作时进行一次combiner，功能上的差距间demo
+ 行动操作：
	+ 行动操作开始执行时才会构建转换算子的DAG图
	+ 行动操作会开启一个Job进行执行(RunJob)


| 函数 | 作用 | 示例 | 结果 |
| --- | --- | --- | --- |
| map | 对RDD中的元素执行函数 | rdd1.map(x => x+1) | {2,3,4,5} |
| flatMap | 对RDD中元素执行函数，将元素数据拆分为迭代器 | rdd1.flatMap(x => 1 to x) | {1,1,1,2,1,2,3,1,2,3,4} |
| filter | 过滤RDD中不符合条件的元素 | rdd1.filter(_ != 1) | {2,3,3} |
| distinct | 去重 | rdd1.distinct | {1,2,3} |
| union | 取两个rdd的合集 | rdd1.union(rdd2) | {1,2,3,3,3,4,5} | 
| intersection | 取两个rdd的交集 | 略 | 略 |
| subtract | 取差集 | rdd1.subtract(rdd2) | {1,2} |
| cartesian | 求笛卡尔积 | rdd1.cartesian(rdd2) | {(1,3),(1,4),(1,5) ···} |
| mappartitions | 对各个分区数据执行函数 | rdd.mapPartitions(num=>{List(num.max).iterator}) | 2,3(按两个分区) |
| glom | 将各个分区数据转化为array类型 | rdd.glom | (1,2) (3,4) |
| sample | 从数据中抽取一些数据，抽取不放回为伯努利算法，否则为泊松算法，一般用来进行数据倾斜处理 | rdd.sample(false,0.5) | {1,3}(采取伯努利算法，0.5代表每个数据的抽取概率) | 
| coalesce(num, shuffle) | 重新分配分区，第二个参数决定是否shuffle |  rdd.coalesce(3, true) | {1,3} {2,5} {4,6}  rdd = (List(1, 2, 3, 4, 5, 6), 2) |
| sortBy | 排序，第二个参数决定升降序 | rdd1.sort | {1,2,3,3} |
| partitionBy | 使用分区器再分区，仅作用于键值类型的RDD | 见demo | 略 |
| reduceByKey | 按照key进行聚合再reduce | 见wordCount | 略 |
| groupByKey | 按照key进行分组，得到(key, CompactBuffer[value])的二元组 | 见demo | 略 |
| aggregateByKey | 将数据按照规则进行分区内计算和分区间计算，第一个参数列表为初始值，第二个参数列表（第一个参数为分区内操作，第二个参数为分区间操作） | 见demo | 略 |
| join | 连接两个rdd，也仅作用于kv RDD | demo | 略 |
| cogroup | 先对两个数据源做一次聚合再，再连接两个rdd，然后再根据结果的key做一次聚合，防止了笛卡尔乘积现象 | demo | 略 |
<center>__表1.RDD转换操作（rdd1={1, 2, 3, 3}，rdd2={3,4,5})__</center>


| 函数 | 作用 | 示例 | 结果 |
| --- | --- | --- | --- |
| collect | 按照分区将不同分区的元素按分区内顺序采集到Driver端 | rdd1.collect | {1,2,3,3} |
| count | rdd元素个数 | rdd1.count | 4 |
| countByValue | 统计rdd中个元素出现次数 | rdd1.countByValue | {(1,1)(2,1)(3,2)} |
| take(num) | 从rdd中取num个元素	，默认takeLeft | rdd1.take | {1,2} |
| top(num) | 从rdd中取最前面n个 | rdd1.top | {3,3} |
| reduce | 并行整合RDD中全部数据 | rdd1.reduce(_ + _) | 9 |
| aggregate | aggergate初始值会参数分区内与分区间运算 | rdd1.aggregate(10)(_ + _, _ + _) | 39 |
| fold | 与reduce一样，不过需要提供初始值（运算区分分区） | rdd1.fold(0)(_ + _) | 9 |
| saveAsTextFile | 以文本形式保存数据 | rdd1.saveAsTextFile(file:///home/test) | |
| saveAsSequenceFile(path) | 将数据集的元素，以顺序文件格式保存到指定的目录下 | saveAsSequenceFile(hdfs://home/test) | |


### RDD序列化
RDD的Operator是在Executor进行执行的，如果需要将数据传递给Executor，则这些数据必须可以进行序列化。
#### 闭包检测
Operator传递的函数包含闭包操作，外部的变量就需要传递给Executor，那么就会进行一次闭包检测，来检测外部变量是否全部序列化，即ClosureCleaner.ensureSerializable方法


### RDD数据执行顺序
RDD是根据分区来决定数据执行顺序，同一个分区内的数据，会一个一个的执行全部计算逻辑
例如分区1中数据有1，2，3，4 ，那么由1执行完全部计算逻辑后，2才会开始执行
分区间数据则是并行执行的

### RDD的数据读取流程
由SparkContext.textFile()中的HadoopRDD方法读取文本数据，每次对RDD进行转换操作（map、join、groupBy等）都会生成新的RDD
### RDD特点
+ 并行度与分区
	+ Spark任务可以被切分为多个子任务，发送给Executor节点并行计算
	+ 并行度可以由用户在读入数据时指定
	+ 也可以使用默认值，默认从配置文件中获取并行度，若没有配置，
	+ 若是从内存中读取数据，则根据totalCores决定
	+ 若从文件中读取数据，则根据minPartions决定，但具体的分区数目是hadoop FileInputFormat中的getSplits方法决定的，这个源码在hadoop部分有解析
+ 仅封装计算逻辑
	+ 数据会在多个转化操作中进行流动，但不会保存在某个确定的RDD
	+ Spark会记录RDD之间的依赖关系，当调用行动操作时，数据才会开始计算 
+ 血缘关系（Lineage链式操作）
	+ 即该RDD是从哪些RDD进行哪些操作得来的，可以类比到.Net的LINQ操作依赖
	+ RDD依赖类型有两种：宽依赖、窄依赖，见图1
		+ 宽依赖，ShuffledRDD，子RDD某个分区需要依赖父RDD的全部分区，这种操作往往需要进行shuffle，例如GroupByKey
		+ 窄依赖，MapPartitionsRDD，每一个父RDD的Partition最多被子RDD的一个PArtition使用，子RDD的分区与父RDD的分区保持一致，分区内数据也是来自父RDD进行操作后的结果
![图1.宽依赖与窄依赖](http://121.4.203.203:8090/upload/2021/10/image-d4bc3978d17b4ed6afa078b4399368c5.png)
+ 分区器（可选）
	+ 分区器仅作用于K-V类型的rdd
+ 首选位置
	+ 对于HDFS文件，判断Task发送到那个节点最优
	+ 因为HDFS文件是存储在不同的节点，Task发送到对应节点可以节省通信开销

### 数据分区的分配
1. 文本数据按行进行读取
2. 每行数据按照偏移量读取
	+ 第二步结束后，相当于获得了偏移量:数据的键值对
3. 数据分区的偏移量范围的计算
	+ 根据总字节数与分区数，决定每个分区数据的偏移量范围
	+ 例如，文件总字节数为10个字节，分区数目为2，那么每个分区5个字节的数据，但是实际存储时，按照[0,5], [5,10]来进行存储

### 阶段&分区&任务

#### 阶段划分
阶段划分是为了进行shuffle，因为需要进行shuffle的算子依赖于全部的分区，那么就无法在下一个算子处并行执行，只有全部分区数据的数据正确的流向下一个算子后才能开始执行。（类似于reduce阶段必须等待map阶段结束）
因此阶段数目 = 宽依赖算子个数 + 1

#### 任务划分
RDD任务切分中间分为：Application、Job、Stage和Task
+ Application ： 初始化一个sparkcontext即一个Application
+ Job ： 每一个Action Op会生成一个Job（job.numPartitions就是最后的一个RDD的分区个数）
+ Stage ： Stage数量相当于宽依赖的个数 + 1
+ Task ： 一个Stage中，最后一个RDD的分区个数就是Task的个数

### RDD持久化
RDD持久化或缓存，可以保存需要重复使用的RDD，减少计算量
持久化方法：cache、persist、checkpoint
+ cache：本质还是调用persist方法，只不过使用默认的仅内存方式存储
	+ cache内存效率高，但是受限于内存
	+ check是通过在血源关系中添加新的依赖实现的，出现问题可以从头读取
+ persist： 可以选择存储策略
	+ 磁盘存储只是临时存储，并非永久存储
+ checkpoint： 保存的数据会保存到磁盘（一般存储到hdfs中）
	+ 为了保证数据安全，会独立执行作业，这句话的意思是Action算子执行时，checkpoint会生成一个独立的job，再跑一遍RDD依赖的血源操作
	+ 由于checkpoint会改变RDD的数据源位置，因此执行过程中，会将血缘关系切断，生成新的血源
	+ checkpoint会重新计算RDD（Action操作），checkpoint效率低，一般和cache一起用（减少一些计算）

一般这三种方法联合使用，例如先进行cache，再进行checkpoint，可以减少一些RDD重新计算

### RDD分区器

## 累加器
分布式共享只写变量

### 累加器应用场景
应用于想要在Executor端执行累加的变量
解释：
+ 常规的变量在Executor端累加后不会传回Driver端，若要在Driver端得到结果，则必须使用collect将数据采集到Driver端再进行运算才可得到结果。
+ 而累加器变量可以实现在分布式端进行运算，将结果传回Driver端，在Driver端进行合并
### Spark默认提供的累加器
longAccumulator
collectionAccumulator
doubleAccumulator

### 可能的异常
+ 少加，没有行动算子，未触发作业导致
+ 多加，多次触发包含累加操作的算子
因此一般将累加器放在行动算子中进行操作


## 广播变量
分布式共享只写变量
### 应用
向Executor发送一个较大的只读值，提供给多个Task使用，例如向Executor发送较大的查询结果提供给不同的Task使用
+ 由于闭包数据以Task为单位发送，导致一个Executor中可能出现大量的重复数据（多个Task使用同一个变量）
+ 广播变量就可以将闭包的数据保存到Executor中

# demo
## Spark程序架构
spark应用应遵循Java三层架构：controller（调度）， service（逻辑）， dao（数据持久化）
### ThreadLocal
在线程内开辟一块用于存放数据的空间，使得各个程序都可以访问到该部分数据

## demo_01
### 商城点击数统计
### 商城各品类用户session的点击统计
### 页面单跳率

